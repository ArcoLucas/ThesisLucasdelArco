{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "106f34b9-557d-47e1-9c17-a46a077e4251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric.graphgym.register as register\n",
    "from torch_geometric.graphgym.config import cfg\n",
    "from torch_geometric.graphgym.models.gnn import GNNPreMP\n",
    "from torch_geometric.graphgym.models.layer import (new_layer_config,BatchNorm1dNode)\n",
    "from torch_geometric.graphgym.register import register_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39e74628-0fb6-486d-8703-1a347df231d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import os\n",
    "import random\n",
    "from torch_geometric.data import Data\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv,  global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6f2c1f57-3557-4d80-ba10-5cc3c6c2b5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load attributes from the whole graph\n",
    "def get_graph_attributes(graph):\n",
    "    \"\"\"Retrieves global graph-level attributes from GraphML metadata.\"\"\"\n",
    "    attributes = {\n",
    "        \"edge_crossings\": graph.graph.get(\"edge_crossings\", 0),\n",
    "        \"diameter\": graph.graph.get(\"diameter\", 0),\n",
    "        \"avg_shortest_path\": graph.graph.get(\"avg_shortest_path\", 0),\n",
    "        \"num_components\": graph.graph.get(\"num_components\", 0),\n",
    "        \"density\": graph.graph.get(\"density\", 0),\n",
    "        \"assortativity\": graph.graph.get(\"assortativity\", 0),\n",
    "    }\n",
    "    return attributes\n",
    "\n",
    "def get_node_features(graph):\n",
    "    \"\"\"Retrieves node attributes from GraphML.\"\"\"\n",
    "    node_features = []\n",
    "    node_mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        x_pos = float(graph.nodes[node].get(\"x_pos\", 0))\n",
    "        y_pos = float(graph.nodes[node].get(\"y_pos\", 0))\n",
    "        degree = graph.degree[node]\n",
    "        clustering = float(graph.nodes[node].get(\"clustering\", 0))\n",
    "        betweenness = float(graph.nodes[node].get(\"betweenness\", 0))\n",
    "        eigenvector = float(graph.nodes[node].get(\"eigenvector\", 0))\n",
    "        pagerank = float(graph.nodes[node].get(\"pagerank\", 0))\n",
    "\n",
    "        node_features.append([x_pos, y_pos, degree, clustering, betweenness, eigenvector, pagerank])\n",
    "\n",
    "    return torch.tensor(node_features, dtype=torch.float), node_mapping\n",
    "\n",
    "def networkx_to_pyg_data(graph):\n",
    "    \"\"\"Converts a NetworkX graph to a PyTorch Geometric Data object with all features.\"\"\"\n",
    "    node_features, node_mapping = get_node_features(graph)\n",
    "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    graph_attrs = get_graph_attributes(graph)\n",
    "    y = torch.tensor([graph_attrs[\"edge_crossings\"]], dtype=torch.float)  # Target variable\n",
    "\n",
    "    return Data(x=node_features, edge_index=edge_index, y=y)\n",
    "\n",
    "def load_graphs(folder_path):\n",
    "    \"\"\"Loads GraphML files and converts them into PyTorch Geometric dataset.\"\"\"\n",
    "    dataset = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".graphml\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                graph = nx.read_graphml(file_path)\n",
    "                pyg_data = networkx_to_pyg_data(graph)\n",
    "                dataset.append(pyg_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "638db0d6-4116-423f-a0a4-8d5fd63ffab2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Module with 'GritTransformer' already defined\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 45\u001b[0m\n\u001b[0;32m     41\u001b[0m             batch \u001b[38;5;241m=\u001b[39m module(batch)\n\u001b[0;32m     42\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m batch\n\u001b[1;32m---> 45\u001b[0m \u001b[38;5;129m@register_network\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGritTransformer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mGritTransformer\u001b[39;00m(torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[0;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;124;03m        The proposed GritTransformer (Graph Inductive Bias Transformer)\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dim_in, dim_out):\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\graphgym\\register.py:40\u001b[0m, in \u001b[0;36mregister_base.<locals>.bounded_register\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mbounded_register\u001b[39m(module):\n\u001b[1;32m---> 40\u001b[0m     register_base(mapping, key, module)\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m module\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch_geometric\\graphgym\\register.py:34\u001b[0m, in \u001b[0;36mregister_base\u001b[1;34m(mapping, key, module)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m mapping:\n\u001b[1;32m---> 34\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule with \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m already defined\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m     mapping[key] \u001b[38;5;241m=\u001b[39m module\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Module with 'GritTransformer' already defined\""
     ]
    }
   ],
   "source": [
    "class FeatureEncoder(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Encoding node and edge features\n",
    "\n",
    "    Args:\n",
    "        dim_in (int): Input feature dimension\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in):\n",
    "        super(FeatureEncoder, self).__init__()\n",
    "        self.dim_in = dim_in\n",
    "        if cfg.dataset.node_encoder:\n",
    "            # print(register.node_encoder_dict)\n",
    "            # Encode integer node features via nn.Embeddings\n",
    "            # print(len(register.node_encoder_dict))\n",
    "            NodeEncoder = register.node_encoder_dict[\n",
    "                cfg.dataset.node_encoder_name]\n",
    "            self.node_encoder = NodeEncoder(cfg.gnn.dim_inner)\n",
    "            if cfg.dataset.node_encoder_bn:\n",
    "                self.node_encoder_bn = BatchNorm1dNode(\n",
    "                    new_layer_config(cfg.gnn.dim_inner, -1, -1, has_act=False,\n",
    "                                     has_bias=False, cfg=cfg))\n",
    "            # Update dim_in to reflect the new dimension fo the node features\n",
    "            self.dim_in = cfg.gnn.dim_inner\n",
    "        if cfg.dataset.edge_encoder:\n",
    "            # Hard-limit max edge dim for PNA.\n",
    "            if 'PNA' in cfg.gt.layer_type:\n",
    "                cfg.gnn.dim_edge = min(128, cfg.gnn.dim_inner)\n",
    "            else:\n",
    "                cfg.gnn.dim_edge = cfg.gnn.dim_inner\n",
    "            # Encode integer edge features via nn.Embeddings\n",
    "            EdgeEncoder = register.edge_encoder_dict[\n",
    "                cfg.dataset.edge_encoder_name]\n",
    "            self.edge_encoder = EdgeEncoder(cfg.gnn.dim_edge)\n",
    "            if cfg.dataset.edge_encoder_bn:\n",
    "                self.edge_encoder_bn = BatchNorm1dNode(\n",
    "                    new_layer_config(cfg.gnn.dim_edge, -1, -1, has_act=False,\n",
    "                                     has_bias=False, cfg=cfg))\n",
    "\n",
    "    def forward(self, batch):\n",
    "        for module in self.children():\n",
    "            batch = module(batch)\n",
    "        return batch\n",
    "\n",
    "\n",
    "@register_network('GritTransformer')\n",
    "class GritTransformer(torch.nn.Module):\n",
    "    '''\n",
    "        The proposed GritTransformer (Graph Inductive Bias Transformer)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        super().__init__()\n",
    "        self.encoder = FeatureEncoder(dim_in)\n",
    "        dim_in = self.encoder.dim_in\n",
    "\n",
    "        self.ablation = True\n",
    "        self.ablation = False\n",
    "\n",
    "        if cfg.posenc_RRWP.enable:\n",
    "            self.rrwp_abs_encoder = register.node_encoder_dict[\"rrwp_linear\"]\\\n",
    "                (cfg.posenc_RRWP.ksteps, cfg.gnn.dim_inner)\n",
    "            rel_pe_dim = cfg.posenc_RRWP.ksteps\n",
    "            self.rrwp_rel_encoder = register.edge_encoder_dict[\"rrwp_linear\"] \\\n",
    "                (rel_pe_dim, cfg.gnn.dim_edge,\n",
    "                 pad_to_full_graph=cfg.gt.attn.full_attn,\n",
    "                 add_node_attr_as_self_loop=False,\n",
    "                 fill_value=0.\n",
    "                 )\n",
    "\n",
    "\n",
    "        if cfg.gnn.layers_pre_mp > 0:\n",
    "            self.pre_mp = GNNPreMP(\n",
    "                dim_in, cfg.gnn.dim_inner, cfg.gnn.layers_pre_mp)\n",
    "            dim_in = cfg.gnn.dim_inner\n",
    "\n",
    "        assert cfg.gt.dim_hidden == cfg.gnn.dim_inner == dim_in, \\\n",
    "            \"The inner and hidden dims must match.\"\n",
    "\n",
    "        global_model_type = cfg.gt.get('layer_type', \"GritTransformer\")\n",
    "        # global_model_type = \"GritTransformer\"\n",
    "\n",
    "        TransformerLayer = register.layer_dict.get(global_model_type)\n",
    "\n",
    "        layers = []\n",
    "        for l in range(cfg.gt.layers):\n",
    "            layers.append(TransformerLayer(\n",
    "                in_dim=cfg.gt.dim_hidden,\n",
    "                out_dim=cfg.gt.dim_hidden,\n",
    "                num_heads=cfg.gt.n_heads,\n",
    "                dropout=cfg.gt.dropout,\n",
    "                sparse=cfg.gt.sparse,\n",
    "                act=cfg.gnn.act,\n",
    "                attn_dropout=cfg.gt.attn_dropout,\n",
    "                layer_norm=cfg.gt.layer_norm,\n",
    "                batch_norm=cfg.gt.batch_norm,\n",
    "                residual=True,\n",
    "                norm_e=cfg.gt.attn.norm_e,\n",
    "                O_e=cfg.gt.attn.O_e,\n",
    "                cfg=cfg.gt,\n",
    "            ))\n",
    "        # layers = []\n",
    "\n",
    "        self.layers = torch.nn.Sequential(*layers)\n",
    "        GNNHead = register.head_dict[cfg.gnn.head]\n",
    "        self.post_mp = GNNHead(dim_in=cfg.gnn.dim_inner, dim_out=dim_out)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        for module in self.children():\n",
    "            batch = module(batch)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b66ae0ab-13d3-42ae-9e57-ba823fcd1a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 328, Validation Samples: 70, Testing Samples: 71\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "posenc_RRWP",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 21\u001b[0m\n\u001b[0;32m     18\u001b[0m num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Modify if needed\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Use GritTransformer instead of GCN\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m model \u001b[38;5;241m=\u001b[39m GritTransformer(dim_in\u001b[38;5;241m=\u001b[39mnum_node_features, dim_out\u001b[38;5;241m=\u001b[39mnum_classes)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Set optimizer and loss function\u001b[39;00m\n\u001b[0;32m     24\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n",
      "Cell \u001b[1;32mIn[11], line 59\u001b[0m, in \u001b[0;36mGritTransformer.__init__\u001b[1;34m(self, dim_in, dim_out)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mablation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mablation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cfg\u001b[38;5;241m.\u001b[39mposenc_RRWP\u001b[38;5;241m.\u001b[39menable:\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrrwp_abs_encoder \u001b[38;5;241m=\u001b[39m register\u001b[38;5;241m.\u001b[39mnode_encoder_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrrwp_linear\u001b[39m\u001b[38;5;124m\"\u001b[39m]\\\n\u001b[0;32m     61\u001b[0m         (cfg\u001b[38;5;241m.\u001b[39mposenc_RRWP\u001b[38;5;241m.\u001b[39mksteps, cfg\u001b[38;5;241m.\u001b[39mgnn\u001b[38;5;241m.\u001b[39mdim_inner)\n\u001b[0;32m     62\u001b[0m     rel_pe_dim \u001b[38;5;241m=\u001b[39m cfg\u001b[38;5;241m.\u001b[39mposenc_RRWP\u001b[38;5;241m.\u001b[39mksteps\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\yacs\\config.py:141\u001b[0m, in \u001b[0;36mCfgNode.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 141\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: posenc_RRWP"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load dataset\n",
    "folder_path = r\"C:\\Users\\lucas\\Desktop\\thesis\\code\\ThesisLucasdelArco\\Data\\rome1_processed\"\n",
    "dataset = load_graphs(folder_path)\n",
    "\n",
    "# Split dataset\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training Samples: {len(train_data)}, Validation Samples: {len(val_data)}, Testing Samples: {len(test_data)}\")\n",
    "\n",
    "# Create Data Loaders\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=4, shuffle=False)\n",
    "\n",
    "# Initialize model\n",
    "num_node_features = dataset[0].x.shape[1]\n",
    "num_classes = 1  # Modify if needed\n",
    "\n",
    "# Use GritTransformer instead of GCN\n",
    "model = GritTransformer(dim_in=num_node_features, dim_out=num_classes)\n",
    "\n",
    "# Set optimizer and loss function\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = nn.HuberLoss()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Pass entire batch through GritTransformer\n",
    "        pred = model(data)  # GritTransformer expects full batch input\n",
    "        loss = loss_fn(pred.squeeze(), data.y)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            pred = model(data).squeeze()\n",
    "            loss = loss_fn(pred, data.y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "plt.plot(range(epochs), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bb26d4-d9b5-498b-85aa-2b4f57cecf59",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
