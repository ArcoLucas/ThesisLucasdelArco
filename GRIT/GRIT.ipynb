{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "468a862e-4325-4a88-ab8d-e4a24fc0af2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_geometric as pyg\n",
    "from torch_geometric.utils.num_nodes import maybe_num_nodes\n",
    "from torch_scatter import scatter, scatter_max, scatter_add\n",
    "from torch_geometric.utils import remove_self_loops, add_remaining_self_loops, add_self_loops\n",
    "#from grit.utils import negate_edge_index\n",
    "from torch_geometric.graphgym.register import *\n",
    "import opt_einsum as oe\n",
    "from yacs.config import CfgNode as CN\n",
    "import warnings\n",
    "#from .GRITSparseConv import GRITSparseConv\n",
    "from torch_geometric.nn.conv import GINEConv, GINConv\n",
    "import networkx as nx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.utils import from_networkx\n",
    "from torch_geometric.loader import DataLoader  # Ensure you use the correct DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3795e3c5-b58d-4d9b-85a1-af52e5263d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import Tensor\n",
    "import os\n",
    "import random\n",
    "from torch_geometric.data import Data\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from torch_geometric.nn import GCNConv,  global_mean_pool\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import r2_score, mean_squared_error, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4eb09b92-63f1-4029-820b-1cf149af0239",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yacs.config import CfgNode as CN\n",
    "\n",
    "def get_default_cfg():\n",
    "    \"\"\"Creates a default configuration object for the GRIT model.\"\"\"\n",
    "    cfg = CN()\n",
    "    cfg.bn_momentum = 0.1 \n",
    "    cfg.bn_no_runner = False\n",
    "    cfg.rezero = False\n",
    "    cfg.update_e = True\n",
    "\n",
    "    cfg.attn = CN()\n",
    "    cfg.attn.use_bias = False\n",
    "    cfg.attn.clamp = 5.0\n",
    "    cfg.attn.act = \"relu\"\n",
    "    cfg.attn.edge_enhance = True\n",
    "    cfg.attn.sqrt_relu = False\n",
    "    cfg.attn.signed_sqrt = False\n",
    "    cfg.attn.scaled_attn = False\n",
    "    cfg.attn.no_qk = False\n",
    "    cfg.attn.use = True\n",
    "    cfg.attn.deg_scaler = True\n",
    "\n",
    "    return cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a66845e0-eb9e-475c-8768-757708ba3ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def networkx_to_pyg_data(graph):\n",
    "    \"\"\"Converts a NetworkX graph to a PyTorch Geometric Data object with all features.\"\"\"\n",
    "    node_features, node_mapping = get_node_features(graph)\n",
    "    edge_index = torch.tensor([[node_mapping[u], node_mapping[v]] for u, v in graph.edges()], dtype=torch.long).t().contiguous()\n",
    "    \n",
    "    graph_attrs = get_graph_attributes(graph)\n",
    "    y = torch.tensor([graph_attrs[\"edge_crossings\"]], dtype=torch.float)  # Target variable\n",
    "\n",
    "    return Data(x=node_features, edge_index=edge_index, y=y)\n",
    "\n",
    "def get_graph_attributes(graph):\n",
    "    \"\"\"Retrieves global graph-level attributes from GraphML metadata.\"\"\"\n",
    "    attributes = {\n",
    "        \"edge_crossings\": graph.graph.get(\"edge_crossings\", 0),\n",
    "        \"diameter\": graph.graph.get(\"diameter\", 0),\n",
    "        \"avg_shortest_path\": graph.graph.get(\"avg_shortest_path\", 0),\n",
    "        \"num_components\": graph.graph.get(\"num_components\", 0),\n",
    "        \"density\": graph.graph.get(\"density\", 0),\n",
    "        \"assortativity\": graph.graph.get(\"assortativity\", 0),\n",
    "    }\n",
    "    return attributes\n",
    "    \n",
    "def get_node_features(graph):\n",
    "    \"\"\"Retrieves node attributes from GraphML.\"\"\"\n",
    "    node_features = []\n",
    "    node_mapping = {node: i for i, node in enumerate(graph.nodes())}\n",
    "\n",
    "    for node in graph.nodes():\n",
    "        x_pos = float(graph.nodes[node].get(\"x_pos\", 0))\n",
    "        y_pos = float(graph.nodes[node].get(\"y_pos\", 0))\n",
    "        degree = graph.degree[node]\n",
    "        clustering = float(graph.nodes[node].get(\"clustering\", 0))\n",
    "        betweenness = float(graph.nodes[node].get(\"betweenness\", 0))\n",
    "        eigenvector = float(graph.nodes[node].get(\"eigenvector\", 0))\n",
    "        pagerank = float(graph.nodes[node].get(\"pagerank\", 0))\n",
    "\n",
    "        node_features.append([x_pos, y_pos, degree, clustering, betweenness, eigenvector, pagerank])\n",
    "\n",
    "    return torch.tensor(node_features, dtype=torch.float), node_mapping\n",
    "    \n",
    "def load_graphs(folder_path):\n",
    "    \"\"\"Loads GraphML files and converts them into PyTorch Geometric dataset.\"\"\"\n",
    "    dataset = []\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".graphml\"):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                graph = nx.read_graphml(file_path)\n",
    "                pyg_data = networkx_to_pyg_data(graph)\n",
    "                dataset.append(pyg_data)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file}: {e}\")\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "    return dataset\n",
    "    \n",
    "# Load dataset\n",
    "folder_path = r\"C:\\Users\\lucas\\Desktop\\thesis\\code\\ThesisLucasdelArco\\Data\\rome1_processed\"\n",
    "dataset = load_graphs(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b915d5be-7e2f-48fe-9965-4711fbe85e2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "b37968a8-55b3-497c-8e39-700f900c0a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 328, Validation Samples: 70, Testing Samples: 71\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch_geometric.data.data.Data'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     49\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m     51\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     52\u001b[0m     data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Move data to GPU/CPU if necessary\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_data()\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_fetcher\u001b[38;5;241m.\u001b[39mfetch(index)  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollate_fn(data)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:316\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    255\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    256\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    258\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    314\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[38;5;241m=\u001b[39mdefault_collate_fn_map)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:191\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    186\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    187\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `copy()` / `__setitem__(index, item)`\u001b[39;00m\n\u001b[0;32m    188\u001b[0m             \u001b[38;5;66;03m# or `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    189\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'torch_geometric.data.data.Data'>"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "folder_path = r\"C:\\Users\\lucas\\Desktop\\thesis\\code\\ThesisLucasdelArco\\Data\\rome1_processed\"\n",
    "dataset = load_graphs(folder_path)\n",
    "\n",
    "# Split dataset\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.3, random_state=42)\n",
    "val_data, test_data = train_test_split(test_data, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f\"Training Samples: {len(train_data)}, Validation Samples: {len(val_data)}, Testing Samples: {len(test_data)}\")\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_data, batch_size=4, shuffle=False)\n",
    "\n",
    "\n",
    "# Initialize the GritTransformer model\n",
    "num_node_features = dataset[0].x.shape[1]\n",
    "\n",
    "cfg = get_default_cfg()  # Get the properly formatted config\n",
    "\n",
    "model = GritTransformerLayer(\n",
    "    in_dim=num_node_features, out_dim=32, num_heads=4,\n",
    "    dropout=0.1, attn_dropout=0.1, residual=True, sparse=False, act='relu',\n",
    "    layer_norm=True, batch_norm=True, cfg=cfg  # Pass the proper config\n",
    ")\n",
    "\n",
    "# Set optimizer and loss function\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "loss_fn = torch.nn.HuberLoss()\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        data = data.to(device)  # Move data to GPU/CPU if necessary\n",
    "        pred = model(data.x, data.edge_index, data.batch)  # âœ… Proper way to pass data\n",
    "        loss = loss_fn(pred.squeeze(), data.y)  # Ensure shapes match\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    train_losses.append(avg_train_loss)\n",
    "\n",
    "    # Validation Loop\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in val_loader:\n",
    "            pred = model(data.x, data.edge_index, data.batch).squeeze()\n",
    "            loss = loss_fn(pred, data.y)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    val_losses.append(avg_val_loss)\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(range(epochs), train_losses, label='Train Loss')\n",
    "plt.plot(range(epochs), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74595cb3-b16d-4621-8912-abefca0cf856",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
